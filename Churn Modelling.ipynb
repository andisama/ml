{
    "nbformat": 4, 
    "cells": [
        {
            "outputs": [], 
            "execution_count": 1, 
            "cell_type": "code", 
            "source": "# Author: Andi Sama\n# Organization: Sinergi Wahana Gemilang\n#   a Value Added Distributor in Jakarta, Indonesia\n# Created: February 7, 2018\n# Last modified: February 8, 2018\n#   - minor changes, typo in comments\n# Topic: A simple step by step to understand deep learning\n# Purpose: Illustration on how to do Deep Learning \n#   - Load existing dataset (data uploaded to IBM DSX Object Storage)\n#   - Data preparation to be ready to be processed by Neural Network\n#     (data encoding for non numbers, split data: train + test + expected output)\n#   - Build, Test Neural Network\n#     build layers, compile, fit, measure accuracy performance\n# Type of Learning: Supervised\n# Platform: Jupyter Notebook with Python 2 with Spark 2.0\n#   on IBM Data Science Experience (on IBM Cloud)\n#     - pandas for data frame manipulation\n#     - numpy for scientific matrix manipulation\n#     - matplotlib for scientific visualization\n# Dataset: https://drive.google.com/file/d/0By9Y49AzZGaUemtpNWtQMWdqRDA/view \n#   structured data with 10,000 records, 14 fields (customer churn)\n# Reference: Build your First Deep Learning Neural Network Model using Keras in Python\n#   https://medium.com/@pushkarmandot/\n#     build-your-first-deep-learning-neural-network-model-using-keras-in-python-a90b5864116d", 
            "metadata": {}
        }, 
        {
            "source": "A. PREPARATION\n\nStep 1. Importing Required Libraries", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 2, 
            "cell_type": "code", 
            "source": "# first overall, importing the required libraries\nimport numpy as np # matrix manipulation\nimport matplotlib.pyplot as plt # visualization\nimport pandas as pd # dataframe\nimport sys # for IBM DSX, accessing dataset in IBM Object Storage\nimport types # for IBM DSX, accessing dataset in IBM Object Storage", 
            "metadata": {}
        }, 
        {
            "source": "B. DATA PRE-PROCESSING\n\nStep 2. Importing required data", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 3, 
            "cell_type": "code", 
            "source": "# The code was removed by DSX for sharing.", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 4, 
            "cell_type": "code", 
            "source": "# dataset.head()\n# print dataset.describe(include=\"all\")\n# print(dataset.head())", 
            "metadata": {}
        }, 
        {
            "source": "Step 3. Prepare training data & target output for Supervised Learning", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 5, 
            "cell_type": "code", 
            "source": "# python indexing starts with 0\n# select all rows for training data (matrix of features)\n#   starting from 3rd index \"Credit Score\" to (max column - 1), meaning excluding \"Exited\"\nX = dataset.iloc[:, 3:13]\n# select all rows for target data (matrix of target output variables)\n#   for last column only, \"Exited\" status\ny = dataset.iloc[:, 13]", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 6, 
            "cell_type": "code", 
            "source": "# print X\n# print y\n# print pd.isnull(X)", 
            "metadata": {}
        }, 
        {
            "source": "Step 4. Encoding string variables", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 7, 
            "cell_type": "code", 
            "source": "# string variables need to be encoded to numeric for further processing\n# in this case, this applies to Geography (\"France\", \"Spain\", ...) & Gender (\"Male\"m \"Female\")\n# tools: use LabelEncoder in ScikitLearn libary, automatic label replacement\n#   with sequence of numeric (starting from 0)\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder # for label encoding", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 8, 
            "cell_type": "code", 
            "source": "# view & verify what we are going to change\n# print(X.iloc[:,1]) # Geography\n# print(X.iloc[:,2]) # Gender", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 9, 
            "cell_type": "code", 
            "source": "labelencoder_X_1 = LabelEncoder()\nX.iloc[:, 1] = labelencoder_X_1.fit_transform(X.iloc[:, 1])\nlabelencoder_X_2 = LabelEncoder()\nX.iloc[:, 2] = labelencoder_X_2.fit_transform(X.iloc[:, 2])", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 10, 
            "cell_type": "code", 
            "source": "onehotencoder = OneHotEncoder(categorical_features = [1])\nX = onehotencoder.fit_transform(X).toarray()", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 11, 
            "cell_type": "code", 
            "source": "X = X[:, 1:]", 
            "metadata": {}
        }, 
        {
            "source": "Step 5. Spliting the dataset into Train & Test (80%:20%)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 12, 
            "cell_type": "code", 
            "source": "from sklearn.cross_validation import train_test_split", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 13, 
            "cell_type": "code", 
            "source": "# the split can also be 70%:30% or even 60%:40%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)", 
            "metadata": {}
        }, 
        {
            "source": "Step 6. Data standarization by scaling (feature scaling)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 14, 
            "cell_type": "code", 
            "source": "from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 15, 
            "cell_type": "code", 
            "source": "X_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)", 
            "metadata": {}
        }, 
        {
            "source": "C. BUILDING NEURAL NETWORK\n\nStep 7. Importing required Neural Network modules", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [
                {
                    "name": "stderr", 
                    "output_type": "stream", 
                    "text": "Using TensorFlow backend.\n"
                }
            ], 
            "execution_count": 16, 
            "cell_type": "code", 
            "source": "# importing sequential module from keras framework\nfrom keras.models import Sequential\nfrom keras.layers import Dense", 
            "metadata": {}
        }, 
        {
            "source": "Step 8. Initializing Neural Network", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 17, 
            "cell_type": "code", 
            "source": "classifier = Sequential()", 
            "metadata": {}
        }, 
        {
            "source": "Step 9. Adding hidden layers to Neural Network", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 18, 
            "cell_type": "code", 
            "source": "# adding the input layer and the 1st hidden layer\nclassifier.add(Dense(output_dim=6, init='uniform', activation='relu', input_dim=11))", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 19, 
            "cell_type": "code", 
            "source": "# adding the 2nd hidden layer\nclassifier.add(Dense(output_dim=6, init='uniform', activation='relu'))", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 20, 
            "cell_type": "code", 
            "source": "# finally, the output layer\nclassifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))", 
            "metadata": {}
        }, 
        {
            "source": "Step 10. Compile the Neural Network", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 21, 
            "cell_type": "code", 
            "source": "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])", 
            "metadata": {}
        }, 
        {
            "source": "Step 11. Train the Neural Network => generates model", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Epoch 1/25\n8000/8000 [==============================] - 1s - loss: 0.4757 - acc: 0.7962     \nEpoch 2/25\n8000/8000 [==============================] - 1s - loss: 0.4253 - acc: 0.7962     \nEpoch 3/25\n8000/8000 [==============================] - 0s - loss: 0.4201 - acc: 0.8000     \nEpoch 4/25\n8000/8000 [==============================] - 0s - loss: 0.4158 - acc: 0.8227     \nEpoch 5/25\n8000/8000 [==============================] - 0s - loss: 0.4136 - acc: 0.8265     \nEpoch 6/25\n8000/8000 [==============================] - 0s - loss: 0.4120 - acc: 0.8300     \nEpoch 7/25\n8000/8000 [==============================] - 0s - loss: 0.4104 - acc: 0.8325     \nEpoch 8/25\n8000/8000 [==============================] - 0s - loss: 0.4094 - acc: 0.8311     \nEpoch 9/25\n8000/8000 [==============================] - 0s - loss: 0.4077 - acc: 0.8304     \nEpoch 10/25\n8000/8000 [==============================] - 0s - loss: 0.4068 - acc: 0.8331     \nEpoch 11/25\n8000/8000 [==============================] - 0s - loss: 0.4058 - acc: 0.8334     \nEpoch 12/25\n8000/8000 [==============================] - 0s - loss: 0.4046 - acc: 0.8334     \nEpoch 13/25\n8000/8000 [==============================] - 0s - loss: 0.4043 - acc: 0.8347     \nEpoch 14/25\n8000/8000 [==============================] - 0s - loss: 0.4038 - acc: 0.8349     \nEpoch 15/25\n8000/8000 [==============================] - 0s - loss: 0.4028 - acc: 0.8355     \nEpoch 16/25\n8000/8000 [==============================] - 0s - loss: 0.4022 - acc: 0.8332     \nEpoch 17/25\n8000/8000 [==============================] - 0s - loss: 0.4020 - acc: 0.8350     \nEpoch 18/25\n8000/8000 [==============================] - 0s - loss: 0.4017 - acc: 0.8336     \nEpoch 19/25\n8000/8000 [==============================] - 0s - loss: 0.4015 - acc: 0.8349     \nEpoch 20/25\n8000/8000 [==============================] - 0s - loss: 0.4013 - acc: 0.8346     \nEpoch 21/25\n8000/8000 [==============================] - 0s - loss: 0.4000 - acc: 0.8356     \nEpoch 22/25\n8000/8000 [==============================] - 0s - loss: 0.4010 - acc: 0.8335     \nEpoch 23/25\n8000/8000 [==============================] - 0s - loss: 0.4005 - acc: 0.8342     \nEpoch 24/25\n8000/8000 [==============================] - 0s - loss: 0.3996 - acc: 0.8341     \nEpoch 25/25\n8000/8000 [==============================] - 0s - loss: 0.3998 - acc: 0.8342     \n"
                }
            ], 
            "execution_count": 22, 
            "cell_type": "code", 
            "source": "# fit the model\nhist = classifier.fit(X_train, y_train, batch_size=10, nb_epoch=25)", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 23, 
            "cell_type": "code", 
            "source": "# print hist.history", 
            "metadata": {}
        }, 
        {
            "source": "Step 12. Predicting the test result", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 24, 
            "cell_type": "code", 
            "source": "classifier.save(\"churn_model_epoch25.hdf5\")", 
            "metadata": {}
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "Epoch 1/25\n8000/8000 [==============================] - 0s - loss: 0.4000 - acc: 0.8364     \nEpoch 2/25\n8000/8000 [==============================] - 0s - loss: 0.3995 - acc: 0.8342     \nEpoch 3/25\n8000/8000 [==============================] - 0s - loss: 0.3993 - acc: 0.8350     \nEpoch 4/25\n8000/8000 [==============================] - 0s - loss: 0.3990 - acc: 0.8360     \nEpoch 5/25\n8000/8000 [==============================] - 0s - loss: 0.3991 - acc: 0.8349     \nEpoch 6/25\n8000/8000 [==============================] - 0s - loss: 0.3991 - acc: 0.8337     \nEpoch 7/25\n8000/8000 [==============================] - 0s - loss: 0.3993 - acc: 0.8362     \nEpoch 8/25\n8000/8000 [==============================] - 0s - loss: 0.3990 - acc: 0.8344     \nEpoch 9/25\n8000/8000 [==============================] - 0s - loss: 0.3991 - acc: 0.8349     \nEpoch 10/25\n8000/8000 [==============================] - 0s - loss: 0.3992 - acc: 0.8351     \nEpoch 11/25\n8000/8000 [==============================] - 0s - loss: 0.3989 - acc: 0.8349     \nEpoch 12/25\n8000/8000 [==============================] - 0s - loss: 0.3984 - acc: 0.8349     \nEpoch 13/25\n8000/8000 [==============================] - 0s - loss: 0.3987 - acc: 0.8357     \nEpoch 14/25\n8000/8000 [==============================] - 0s - loss: 0.3987 - acc: 0.8364     \nEpoch 15/25\n8000/8000 [==============================] - 0s - loss: 0.3983 - acc: 0.8366     \nEpoch 16/25\n8000/8000 [==============================] - 0s - loss: 0.3983 - acc: 0.8344     \nEpoch 17/25\n8000/8000 [==============================] - 0s - loss: 0.3982 - acc: 0.8369     \nEpoch 18/25\n8000/8000 [==============================] - 0s - loss: 0.3979 - acc: 0.8350     \nEpoch 19/25\n8000/8000 [==============================] - 0s - loss: 0.3984 - acc: 0.8352     \nEpoch 20/25\n8000/8000 [==============================] - 0s - loss: 0.3979 - acc: 0.8352     \nEpoch 21/25\n8000/8000 [==============================] - 0s - loss: 0.3984 - acc: 0.8362     \nEpoch 22/25\n8000/8000 [==============================] - 0s - loss: 0.3978 - acc: 0.8350     \nEpoch 23/25\n8000/8000 [==============================] - 0s - loss: 0.3982 - acc: 0.8356     \nEpoch 24/25\n8000/8000 [==============================] - 0s - loss: 0.3981 - acc: 0.8360     \nEpoch 25/25\n8000/8000 [==============================] - 0s - loss: 0.3979 - acc: 0.8354     \n"
                }
            ], 
            "execution_count": 25, 
            "cell_type": "code", 
            "source": "hist = classifier.fit(X_train, y_train, batch_size=10, nb_epoch=25)", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 26, 
            "cell_type": "code", 
            "source": "classifier.save(\"churn_model_epoch50.hdf5\")", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 27, 
            "cell_type": "code", 
            "source": "# print hist.history", 
            "metadata": {}
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "-rw------- 1 s442-595b4f813a3de1-85d300ba5a34 users 19976 Feb  7 09:35 churn_model_epoch25.hdf5\r\n-rw------- 1 s442-595b4f813a3de1-85d300ba5a34 users 19976 Feb  7 09:35 churn_model_epoch50.hdf5\r\n"
                }
            ], 
            "execution_count": 28, 
            "cell_type": "code", 
            "source": "# verify that models have been generated \n!ls -al *.hdf5", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 29, 
            "cell_type": "code", 
            "source": "# predicting the test set results\ny_pred = classifier.predict(X_test)\ny_pred = (y_pred > 0.5)", 
            "metadata": {}
        }, 
        {
            "source": "Step 13. Evaluating model performance (accuracy)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 30, 
            "cell_type": "code", 
            "source": "# creating confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)", 
            "metadata": {}
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "[[1569   24]\n [ 302  105]]\n"
                }
            ], 
            "execution_count": 31, 
            "cell_type": "code", 
            "source": "#confusion matrix\n#            event           no-event\n# event      true positive   false positive\n# no-event   false negative   true negative\nprint cm\n\n# calculating model performance from the confusion matrix\ncm_truepositive = float(cm[0,0])\ncm_truenegative = float(cm[1,1])\ncm_total = float(cm[0,0] + cm[0,1] + cm [1,0] + cm [1,1])\ncm_accuracy = float((cm_truepositive + cm_truenegative) / cm_total)", 
            "metadata": {}
        }, 
        {
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "83.7\n"
                }
            ], 
            "execution_count": 32, 
            "cell_type": "code", 
            "source": "print (cm_accuracy*100)", 
            "metadata": {}
        }, 
        {
            "source": "Step 14. Visualizing Results", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 33, 
            "cell_type": "code", 
            "source": "# can use pyplot here to do some visualization as needed", 
            "metadata": {}
        }, 
        {
            "source": "Step 15. Loading \"saved model\" to be used in the application", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": 34, 
            "cell_type": "code", 
            "source": "# 1st approach to load saved model\n# from keras.models import load_model\n# classifier.load_model(\"churn_model_epoch50.hdf5\")\n# ...\n# use model here...\n# ...\n#\n# alternatively can use the following approach:\n# import h5py\n# with h5py.File('churn_model_epoch50.hdf5', 'r') as f:\n#    x_data = f['x_data']\n#    model_variable.predict(x_data)", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython2", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "name": "python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.0", 
            "name": "python2-spark20"
        }
    }, 
    "nbformat_minor": 2
}